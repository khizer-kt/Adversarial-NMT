{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEbfs1RagoiL"
      },
      "outputs": [],
      "source": [
        "dataset_link = 'https://www.kaggle.com/datasets/zainuddin123/parallel-corpus-for-english-urdu-language'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/english-corpus.txt', 'r') as file:\n",
        "    english_rows = file.readlines()\n",
        "english_row_count = len(english_rows)\n",
        "\n",
        "with open('/content/urdu-corpus.txt', 'r') as file:\n",
        "    urdu_rows = file.readlines()\n",
        "urdu_row_count = len(urdu_rows)\n",
        "\n",
        "\n",
        "print(f\"Urdu: {urdu_row_count}\")\n",
        "print(f\"English: {english_row_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fZchsBWk36m",
        "outputId": "944379dc-c381-4b83-9495-9326ee83a76a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urdu: 24525\n",
            "English: 24525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working"
      ],
      "metadata": {
        "id": "KGfpFd65liRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "with open('/content/english-corpus.txt', 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "\n",
        "with open('/content/urdu-corpus.txt', 'r') as file:\n",
        "    urdu_sentences = file.readlines()\n",
        "\n",
        "def preprocess(sentences):\n",
        "    sentences = [s.strip().lower() for s in sentences]\n",
        "    return sentences\n",
        "\n",
        "english_sentences = preprocess(english_sentences)\n",
        "urdu_sentences = preprocess(urdu_sentences)\n",
        "\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "urdu_tokenizer = Tokenizer()\n",
        "urdu_tokenizer.fit_on_texts(urdu_sentences)\n",
        "urdu_sequences = urdu_tokenizer.texts_to_sequences(urdu_sentences)\n",
        "\n",
        "max_english_length = max([len(seq) for seq in english_sequences])\n",
        "max_urdu_length = max([len(seq) for seq in urdu_sequences])\n",
        "english_padded = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n",
        "urdu_padded = pad_sequences(urdu_sequences, maxlen=max_urdu_length, padding='post')\n",
        "english_padded = np.array(english_padded)\n",
        "urdu_padded = np.array(urdu_padded)"
      ],
      "metadata": {
        "id": "8f54i7gVonn2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(len(english_tokenizer.word_index) + 1, latent_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(len(urdu_tokenizer.word_index) + 1, latent_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(urdu_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fpVkcAdtgqj",
        "outputId": "f74f8e08-bc3d-4c90-bd23-04d2bce5ed79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 256)            1453824   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 256)            1518080   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 256),                525312    ['embedding[0][0]']           \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 256),          525312    ['embedding_1[0][0]',         \n",
            "                              (None, 256),                           'lstm[0][1]',                \n",
            "                              (None, 256)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 5930)           1524010   ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5546538 (21.16 MB)\n",
            "Trainable params: 5546538 (21.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urdu_padded_shifted = np.zeros_like(urdu_padded)\n",
        "urdu_padded_shifted[:, :-1] = urdu_padded[:, 1:]\n",
        "\n",
        "model.fit([english_padded, urdu_padded], urdu_padded_shifted,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQeNEaoctisR",
        "outputId": "36c8dda4-4fa1-4eef-a86f-c8627378b63a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "307/307 [==============================] - 14s 31ms/step - loss: 1.6255 - val_loss: 1.3408\n",
            "Epoch 2/20\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 1.3104 - val_loss: 1.2913\n",
            "Epoch 3/20\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 1.2619 - val_loss: 1.2471\n",
            "Epoch 4/20\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 1.2223 - val_loss: 1.2282\n",
            "Epoch 5/20\n",
            "307/307 [==============================] - 4s 13ms/step - loss: 1.1823 - val_loss: 1.1784\n",
            "Epoch 6/20\n",
            "307/307 [==============================] - 4s 13ms/step - loss: 1.1389 - val_loss: 1.1352\n",
            "Epoch 7/20\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 1.0986 - val_loss: 1.1228\n",
            "Epoch 8/20\n",
            "307/307 [==============================] - 4s 13ms/step - loss: 1.0622 - val_loss: 1.0764\n",
            "Epoch 9/20\n",
            "307/307 [==============================] - 4s 13ms/step - loss: 1.0296 - val_loss: 1.0458\n",
            "Epoch 10/20\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.9975 - val_loss: 1.0173\n",
            "Epoch 11/20\n",
            "307/307 [==============================] - 4s 13ms/step - loss: 0.9665 - val_loss: 1.0131\n",
            "Epoch 12/20\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.9360 - val_loss: 0.9707\n",
            "Epoch 13/20\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 0.9075 - val_loss: 0.9520\n",
            "Epoch 14/20\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 0.8819 - val_loss: 0.9339\n",
            "Epoch 15/20\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.8574 - val_loss: 0.9208\n",
            "Epoch 16/20\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 0.8333 - val_loss: 0.9092\n",
            "Epoch 17/20\n",
            "307/307 [==============================] - 4s 13ms/step - loss: 0.8111 - val_loss: 0.8932\n",
            "Epoch 18/20\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.7901 - val_loss: 0.8772\n",
            "Epoch 19/20\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 0.7696 - val_loss: 0.8649\n",
            "Epoch 20/20\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.7494 - val_loss: 0.8502\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7945c0b12c50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial Training\n",
        "from tensorflow.keras.layers import GRU, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "discriminator = tf.keras.Sequential([\n",
        "    Embedding(len(urdu_tokenizer.word_index) + 1, latent_dim),\n",
        "    GRU(latent_dim, return_sequences=True),\n",
        "    TimeDistributed(Dense(1, activation='sigmoid'))\n",
        "])\n",
        "\n",
        "discriminator.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
        "\n",
        "def train_discriminator(model, english_data, urdu_data, batch_size=64):\n",
        "    translated_urdu = model.predict([english_data, urdu_data])\n",
        "    real_labels = np.ones((batch_size, max_urdu_length, 1))\n",
        "    fake_labels = np.zeros((batch_size, max_urdu_length, 1))\n",
        "    discriminator.train_on_batch(urdu_data, real_labels)\n",
        "    discriminator.train_on_batch(translated_urdu, fake_labels)\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.fit([english_padded, urdu_padded], urdu_padded_shifted, batch_size=64, epochs=1)\n",
        "    train_discriminator(model, english_padded, urdu_padded)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} completed\")"
      ],
      "metadata": {
        "id": "klzdqEJVwyPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate_model(model, input_seq, target_seq):\n",
        "    prediction = model.predict(input_seq)\n",
        "    predicted_seq = np.argmax(prediction, axis=-1)\n",
        "    for i in range(len(input_seq)):\n",
        "        input_sentence = ' '.join([english_tokenizer.index_word[word] for word in input_seq[i] if word != 0])\n",
        "        target_sentence = ' '.join([urdu_tokenizer.index_word[word] for word in target_seq[i] if word != 0])\n",
        "        predicted_sentence = ' '.join([urdu_tokenizer.index_word[word] for word in predicted_seq[i] if word != 0])\n",
        "        print(f\"Input: {input_sentence}\")\n",
        "        print(f\"Target: {target_sentence}\")\n",
        "        print(f\"Predicted: {predicted_sentence}\")\n",
        "        print(f\"BLEU score: {sentence_bleu([target_sentence.split()], predicted_sentence.split())}\")\n",
        "        print(\"\")\n",
        "\n",
        "evaluate_model(model, english_padded[:10], urdu_padded[:10])"
      ],
      "metadata": {
        "id": "Kxve-lS_w8Ho"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}